{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic: Kaggle Campaign - Fake News Detection Challenge KDD 2020\n",
    "Description: Using LightGBM and XGBoodt model to detect the fake news. Please refer to the dataset: https://www.kaggle.com/c/fakenewskdd2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Import the needed tools and datasets\n",
    "Firstly, import all tools and dataset which are needed, including the the tools for manage dataframe, array, and the tools for characters transform to vectors, and the tools for model construct from scikit-learn. Also, import the score to measure the model performance. \n",
    "____\n",
    "After loading the data, we can see there are only two columns, one is \"Text\", which is the Variable X. And the other column is \"Label\", when 1 representing Fake, and 0 representing True, which is the Target Variable, Value Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>The storybook romance of WWE stars John Cena a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>The actor told friends he’s responsible for en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>Sarah Hyland is getting real.  The Modern Fami...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>Production has been suspended on the sixth and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>A jury ruled against Bill Cosby in his sexual ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4987 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Get the latest from TODAY Sign up for our news...     1\n",
       "1     2d  Conan On The Funeral Trump Will Be Invited...     1\n",
       "2     It’s safe to say that Instagram Stories has fa...     0\n",
       "3     Much like a certain Amazon goddess with a lass...     0\n",
       "4     At a time when the perfect outfit is just one ...     0\n",
       "...                                                 ...   ...\n",
       "4982  The storybook romance of WWE stars John Cena a...     0\n",
       "4983  The actor told friends he’s responsible for en...     0\n",
       "4984  Sarah Hyland is getting real.  The Modern Fami...     0\n",
       "4985  Production has been suspended on the sixth and...     0\n",
       "4986  A jury ruled against Bill Cosby in his sexual ...     0\n",
       "\n",
       "[4987 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load datasets\n",
    "df_train = pd.read_csv(\"train.csv\", \"\\t\",encoding='utf-8',header=(0))\n",
    "df_test = pd.read_csv(\"test.csv\", \"\\t\",encoding='utf-8',header=(0))\n",
    "df_sub = pd.read_csv(\"sample_submission.csv\",encoding='utf-8',header=(0))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Data Pre-processing\n",
    "Seperate X and Y, Train and Test. After setting Stop words to get rid of the meanless words, we can transfer the text to vectors to caculate their features. Take the 1800 features only with the largest TF-IDF value among all features, which are Variable X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set x&y train and test\n",
    "x_train = df_train['text']\n",
    "y_train = df_train['label'].tolist()\n",
    "x_test = df_test['text']\n",
    "y_test=pd.to_numeric(df_sub['label']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set stop words\n",
    "stopwords= text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform text to vector by Tfidf\n",
    "vectorizer = TfidfVectorizer(\n",
    "            norm='l2',                      \n",
    "            stop_words=stopwords,\n",
    "            max_features=1800               \n",
    "            )\n",
    "\n",
    "X_train = vectorizer.fit_transform(x_train).toarray()\n",
    "X_test = vectorizer.fit_transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Construct Model\n",
    "In this case, we use two models, XGBoost and LightGBM. After trying several times, set the parameters of this model.\n",
    "### XGBoost\n",
    "1. The learning rate of gradient descent is set to 0.5, which is common seen.  (Have tried 0.1 is worse.)\n",
    "2. The number of trees is set to 100 (I found that the accuracy did not improve with more trees)\n",
    "3. The depth of the tree is set to 6 layers (10 layers is too much, 5 layers is too little)\n",
    "4. Specify the loss function as binary classification of logistic regression.\n",
    "___\n",
    "### LightGBM\n",
    "1. The learning rate of gradient descent is set to 0.5, which is common seen. (Have tried 0.1 is worse.)\n",
    "2. The number of leaves of the tree is set to 50 (100 is too high)\n",
    "3. The number of trees is set to 120 (120 is a little better than 100)\n",
    "4. Limit the depth of the tree to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:51:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#applying Xgboost model\n",
    "\n",
    "#set paramaters\n",
    "XGB_Classfier = xgb.XGBClassifier(learning_rate=0.5,                   \n",
    "                              n_estimators=100,         \n",
    "                              max_depth=6,                  \n",
    "                              gamma=5,                               \n",
    "                              objective='binary:logistic',\n",
    "                              random_state=99            \n",
    "                              )\n",
    "#training model\n",
    "XGB_Classfier = XGB_Classfier.fit(X_train, y_train)\n",
    "#predicting\n",
    "Xgb_pred = XGB_Classfier.predict(X_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.530072\n",
      "Precision: 0.540052\n",
      "Recall: 0.338736\n",
      "F_measure: 0.416335\n"
     ]
    }
   ],
   "source": [
    "#reviewing model performance\n",
    "Xgb_accuracy = accuracy_score(y_test, Xgb_pred)\n",
    "Xgb_precision = metrics.precision_score(y_test, Xgb_pred)\n",
    "Xgb_recall = metrics.recall_score(y_test, Xgb_pred)\n",
    "Xgb_F_measure = metrics.f1_score(y_test, Xgb_pred)\n",
    "\n",
    "print(\"Accuracy: %f\" % Xgb_accuracy)\n",
    "print(\"Precision: %f\" % Xgb_precision)\n",
    "print(\"Recall: %f\" % Xgb_recall)\n",
    "print(\"F_measure: %f\" % Xgb_F_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.72      0.61       630\n",
      "           1       0.54      0.34      0.42       617\n",
      "\n",
      "    accuracy                           0.53      1247\n",
      "   macro avg       0.53      0.53      0.51      1247\n",
      "weighted avg       0.53      0.53      0.51      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGBC_report = classification_report(y_test, Xgb_pred)\n",
    "print(XGBC_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying LightGBM model\n",
    "\n",
    "#set paramaters\n",
    "LGB_Classifier = lgb.LGBMClassifier( \n",
    "                      learning_rate=0.5, \n",
    "                      num_leaves=50,\n",
    "                      n_estimators=120,\n",
    "                      max_bin=200,\n",
    "                      random_state=99,          \n",
    "                      device='cpu'\n",
    "                      )\n",
    "#training model\n",
    "LGB_Classfier = LGB_Classifier.fit(X_train, y_train)\n",
    "#predicting\n",
    "Lgb_pred = LGB_Classfier.predict(X_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.512430\n",
      "Precision: 0.514019\n",
      "Recall: 0.267423\n",
      "F_measure: 0.351812\n"
     ]
    }
   ],
   "source": [
    "#reviewing model performance\n",
    "Lgb_accuracy = accuracy_score(y_test, Lgb_pred)\n",
    "Lgb_precision = metrics.precision_score(y_test, Lgb_pred)\n",
    "Lgb_recall = metrics.recall_score(y_test, Lgb_pred)\n",
    "Lgb_F_measure = metrics.f1_score(y_test, Lgb_pred)\n",
    "\n",
    "print(\"Accuracy: %f\" % Lgb_accuracy)\n",
    "print(\"Precision: %f\" % Lgb_precision)\n",
    "print(\"Recall: %f\" % Lgb_recall)\n",
    "print(\"F_measure: %f\" % Lgb_F_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.75      0.61       630\n",
      "           1       0.51      0.27      0.35       617\n",
      "\n",
      "    accuracy                           0.51      1247\n",
      "   macro avg       0.51      0.51      0.48      1247\n",
      "weighted avg       0.51      0.51      0.48      1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LGBC_report = classification_report(y_test, Lgb_pred)\n",
    "print(LGBC_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
